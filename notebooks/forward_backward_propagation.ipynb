{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c12871f-7b5e-49d0-ac7f-9fa3db40a74e",
   "metadata": {},
   "source": [
    "# Forward y Back propagation en Deep Learning\n",
    "Implementaremos paso a paso tanto la propagación hacia adelante como hacia atrás.\n",
    "\n",
    "En este notebook implementamos:\n",
    "\n",
    "1. Forward Propagation:\n",
    "   - Calcula las activaciones capa por capa\n",
    "   - Usa ReLU para capas ocultas y Sigmoid para la capa de salida\n",
    "   - Almacena valores intermedios en cache para backward propagation\n",
    "\n",
    "2. Función de Costo:\n",
    "   - Implementa binary cross-entropy\n",
    "   - Incluye epsilon para evitar problemas numéricos con log(0)\n",
    "\n",
    "3. Backward Propagation:\n",
    "   - Calcula gradientes para cada capa\n",
    "   - Implementa las fórmulas de la regla de la cadena\n",
    "   - Calcula gradientes para pesos (W) y sesgos (b)\n",
    "\n",
    "4. Actualización de Parámetros:\n",
    "   - Implementa el descenso del gradiente\n",
    "   - Actualiza W y b usando los gradientes calculados\n",
    "\n",
    "También incluimos código de prueba para verificar las dimensiones y el funcionamiento básico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdf9066f-9b3f-4820-b940-49ed6dfd6d92",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mft_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m relu, relu_derivative, sigmoid, sigmoid_derivative\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward_propagation\u001b[39m(X, parameters):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    Implementa la propagación hacia adelante\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m        tuple: Predicción final (Y_hat)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.ft_functions import relu, relu_derivative, sigmoid, sigmoid_derivative\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia adelante\n",
    "    \n",
    "    Args:\n",
    "        X (numpy.ndarray): Datos de entrada (n_features, n_samples)\n",
    "        parameters (dict): Diccionario con los parámetros W1, b1, W2, b2, W3, b3\n",
    "    \n",
    "    Returns:\n",
    "        dict: Diccionario con las activaciones y valores Z de cada capa\n",
    "        tuple: Predicción final (Y_hat)\n",
    "    \"\"\"\n",
    "    cache = {}\n",
    "    \n",
    "    # Primera capa oculta\n",
    "    Z1 = np.dot(parameters['W1'], X) + parameters['b1']\n",
    "    A1 = relu(Z1)\n",
    "    cache['Z1'] = Z1\n",
    "    cache['A1'] = A1\n",
    "    \n",
    "    # Segunda capa oculta\n",
    "    Z2 = np.dot(parameters['W2'], A1) + parameters['b2']\n",
    "    A2 = relu(Z2)\n",
    "    cache['Z2'] = Z2\n",
    "    cache['A2'] = A2\n",
    "    \n",
    "    # Capa de salida\n",
    "    Z3 = np.dot(parameters['W3'], A2) + parameters['b3']\n",
    "    A3 = sigmoid(Z3)  # Usamos sigmoid para la salida binaria\n",
    "    cache['Z3'] = Z3\n",
    "    cache['A3'] = A3\n",
    "    \n",
    "    return cache, A3\n",
    "\n",
    "def compute_cost(A3, Y):\n",
    "    \"\"\"\n",
    "    Calcula el costo usando binary cross-entropy\n",
    "    \n",
    "    Args:\n",
    "        A3 (numpy.ndarray): Salida de la red (predicciones)\n",
    "        Y (numpy.ndarray): Valores reales\n",
    "    \n",
    "    Returns:\n",
    "        float: Costo calculado\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # Añadimos epsilon para evitar log(0)\n",
    "    epsilon = 1e-15\n",
    "    A3 = np.clip(A3, epsilon, 1 - epsilon)\n",
    "    \n",
    "    cost = -(1/m) * np.sum(Y * np.log(A3) + (1 - Y) * np.log(1 - A3))\n",
    "    \n",
    "    return float(cost)\n",
    "\n",
    "def backward_propagation(X, Y, parameters, cache):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia atrás\n",
    "    \n",
    "    Args:\n",
    "        X (numpy.ndarray): Datos de entrada\n",
    "        Y (numpy.ndarray): Valores reales\n",
    "        parameters (dict): Parámetros de la red\n",
    "        cache (dict): Valores almacenados del forward propagation\n",
    "    \n",
    "    Returns:\n",
    "        dict: Gradientes para cada parámetro\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    gradients = {}\n",
    "    \n",
    "    # Gradientes de la capa de salida\n",
    "    dZ3 = cache['A3'] - Y\n",
    "    gradients['dW3'] = (1/m) * np.dot(dZ3, cache['A2'].T)\n",
    "    gradients['db3'] = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    \n",
    "    # Gradientes de la segunda capa oculta\n",
    "    dA2 = np.dot(parameters['W3'].T, dZ3)\n",
    "    dZ2 = dA2 * relu_derivative(cache['Z2'])\n",
    "    gradients['dW2'] = (1/m) * np.dot(dZ2, cache['A1'].T)\n",
    "    gradients['db2'] = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    # Gradientes de la primera capa oculta\n",
    "    dA1 = np.dot(parameters['W2'].T, dZ2)\n",
    "    dZ1 = dA1 * relu_derivative(cache['Z1'])\n",
    "    gradients['dW1'] = (1/m) * np.dot(dZ1, X.T)\n",
    "    gradients['db1'] = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    Actualiza los parámetros usando descenso del gradiente\n",
    "    \n",
    "    Args:\n",
    "        parameters (dict): Parámetros actuales\n",
    "        gradients (dict): Gradientes calculados\n",
    "        learning_rate (float): Tasa de aprendizaje\n",
    "    \n",
    "    Returns:\n",
    "        dict: Parámetros actualizados\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2  # Número de capas\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        parameters[f'W{l}'] -= learning_rate * gradients[f'dW{l}']\n",
    "        parameters[f'b{l}'] -= learning_rate * gradients[f'db{l}']\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# Ejemplo de uso con datos pequeños para verificar dimensiones\n",
    "np.random.seed(42)\n",
    "X_sample = np.random.randn(30, 5)  # 5 muestras con 30 características\n",
    "Y_sample = np.random.randint(0, 2, (1, 5))  # 5 etiquetas binarias\n",
    "\n",
    "# Inicializar parámetros para el ejemplo\n",
    "layer_dims = [30, 16, 8, 1]\n",
    "parameters = {\n",
    "    'W1': np.random.randn(16, 30) * np.sqrt(2./30),\n",
    "    'b1': np.zeros((16, 1)),\n",
    "    'W2': np.random.randn(8, 16) * np.sqrt(2./16),\n",
    "    'b2': np.zeros((8, 1)),\n",
    "    'W3': np.random.randn(1, 8) * np.sqrt(2./8),\n",
    "    'b3': np.zeros((1, 1))\n",
    "}\n",
    "\n",
    "# Probar forward propagation\n",
    "cache, A3 = forward_propagation(X_sample, parameters)\n",
    "\n",
    "# Calcular costo\n",
    "cost = compute_cost(A3, Y_sample)\n",
    "print(f\"Costo inicial: {cost}\")\n",
    "\n",
    "# Probar backward propagation\n",
    "gradients = backward_propagation(X_sample, Y_sample, parameters, cache)\n",
    "\n",
    "# Actualizar parámetros\n",
    "learning_rate = 0.01\n",
    "parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "\n",
    "# Verificar las dimensiones\n",
    "print(\"\\nDimensiones de las matrices:\")\n",
    "print(f\"X: {X_sample.shape}\")\n",
    "print(f\"Y: {Y_sample.shape}\")\n",
    "for key, value in parameters.items():\n",
    "    print(f\"{key}: {value.shape}\")\n",
    "for key, value in gradients.items():\n",
    "    print(f\"{key}: {value.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
