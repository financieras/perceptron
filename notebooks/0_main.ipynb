{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce3c61b8-2237-46ae-82e5-fa4cb460d291",
   "metadata": {},
   "source": [
    "# Proyecto de Aprendizaje Automático\n",
    "## Perceptrón Multicapa\n",
    "Este proyecto es una introducción a las redes neuronales artificiales: con la implementación de perceptrones multicapa.  \n",
    "Se utilizan los datos del Wisconsin Breast Cancer Dataset (WBCD)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16c31a7-cb77-401d-b7d3-aa4154f575de",
   "metadata": {},
   "source": [
    "## Estructura de ficheros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baef819-8ac5-4864-a0a5-ed0cf62534ed",
   "metadata": {},
   "source": [
    "```bash\n",
    "/perceptron\n",
    "│\n",
    "├── data/                              # Carpeta para datos crudos y preprocesados\n",
    "│   ├── raw/                           # Datos originales\n",
    "│   │   └── data.csv                   # Archivo de datos principal\n",
    "│   └── processed/                     # Datos preprocesados y normalizados\n",
    "│       ├─── log_normalized.csv        # Dataset normalizado con Log Transform\n",
    "│       ├─── minmax_normalized.csv     # Dataset normalizado con Min-Max Scaling\n",
    "│       ├─── robust_normalized.csv     # Dataset normalizado con Rotust Scaling\n",
    "│       └─── zscore_normalized.csv     # Dataset normalizado con Z-score Normalization\n",
    "├── notebooks/                         # Archivos Jupyter Notebook\n",
    "│   ├── 0_main.ipynb                   # Orquestador principal\n",
    "│   ├── 1_data_analysis.ipynb          # Análisis de datos\n",
    "│   ├── 2_normalization.ipynb          # Normalización de datos\n",
    "│   ├── 3_data_visualization.ipynb     # Visualización de datos\n",
    "│   ├── 4_preprocessing_data.ipynb     # Preprocesamiento de datos (división datos entrenamiento / validación)\n",
    "│   ├── 5_model_training.ipynb         # Entrenamiento del modelo\n",
    "│   ├── model_training.ipynb           # Entrenamiento del modelo\n",
    "│   ├── prediction.ipynb               # Predicción y evaluación\n",
    "│   └── documentation.ipynb            # Documentación de funciones\n",
    "├── output/                            # Resultados del proyecto\n",
    "│   ├── figures/                       # Imágenes y gráficos\n",
    "│   │   ├──── boxplot_features.png     # Gráfico de caja (boxplots)\n",
    "│   │   ├──── correlation_heatmap.png  # Gráfico Heatmap: mapa de calor para correlaciones\n",
    "│   │   ├──── feature_importance.png   # Gráfico de Barras de Importancia de Característic\n",
    "│   │   ├──── pairplot_features.png    # Gráfico de Pares para las características más relevantes\n",
    "│   │   ├──── pca_visualization.png    # Gráfico para ver la separación de clases\n",
    "│   │   ├──── radar_plot.png           # Gráfico de radar\n",
    "│   │   ├──── violin_plot_log.png      # Gráfico de violín con normalización log\n",
    "│   │   ├──── violin_plot_minmax.png   # Gráfico de violín con normalización min-max\n",
    "│   │   ├──── violin_plot_robust.png   # Gráfico de violín con normalización robusta\n",
    "│   │   └──── violin_plot_zscore.png   # Gráfico de violín con normalización z-score\n",
    "│   ├── models/                        # Resultados del modelo\n",
    "│   └── features.json                  # JSON con las distribuciones de probabilidad\n",
    "│   └── normalization_params.json      # Medias y desviacines de la normalización del conjunto de entrenamiento\n",
    "├── utils/                             # Funciones reutilizables\n",
    "│   ├── config.py                      # Hiperparámetros y configuraciones\n",
    "│   ├── neural_network_functions.py    # Funciones para el modelo\n",
    "│   ├── statistical_functions.py       # Funciones estadísticas\n",
    "│   ├── preprocessing_functions.py     # Funciones de preprocesamiento\n",
    "│   └── evaluation_functions.py        # Funciones de evaluación\n",
    "├── requirements.txt                   # Librerías usadas con Pip\n",
    "└── README.md                          # Instalación e Introducción al proyecto\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6380464-a7ae-4b56-94c5-ecf71d4ff186",
   "metadata": {},
   "source": [
    "## Estructura de los Notebooks\n",
    "\n",
    "0. main.ipynb\n",
    "    - Punto de entrada del proyecto y\n",
    "    - el \"orquestador\" del proyecto\n",
    "    - Definimos los hiperparámetros y configuraciones\n",
    "    - Llamamos a funciones\n",
    "      \n",
    "2. **Análisis de datos**: \n",
    "   - [1 data_analysis.ipynb](1_data_analysis.ipynb)\n",
    "   - Primer paso para entender los datos\n",
    "   - Análisis estadístico inicial\n",
    "   - Identificación de características\n",
    "\n",
    "4. data_visualization.ipynb\n",
    "   - Visualización de distribuciones\n",
    "   - Gráficos exploratorios\n",
    "   - Comprensión visual de los datos\n",
    "\n",
    "5. data_preprocessing.ipynb\n",
    "   - Limpieza de datos\n",
    "   - Normalización\n",
    "       - Parámetro: diferentes métodos de normalización \n",
    "   - Separación de datos en entrenamiento y validación\n",
    "       - Parámetro: el porcentaje de división train/val\n",
    "   - Preparación para modelado\n",
    "\n",
    "6. model_training.ipynb\n",
    "   - Construcción del modelo\n",
    "   - Función flexible para construir el modelo que acepte parámetros como:\n",
    "        - Parámetro: número de capas\n",
    "        - Parámetro: neuronas por capa\n",
    "        - Parámetro: funciones de activación\n",
    "        - Parámetro: diferentes inicializadores de pesos\n",
    "        - Parámetro: tasa de aprendizaje\n",
    "        - Parámetro: algoritmo de optimización\n",
    "        - Parámetro: number of Epochs\n",
    "   - Entrenamiento de la red neuronal\n",
    "\n",
    "7. prediction.ipynb\n",
    "   - Evaluación final\n",
    "   - Predicciones\n",
    "   - Métricas de rendimiento\n",
    "   - Funciones para visualizar y comparar resultados de diferentes configuraciones\n",
    "\n",
    "\n",
    "* Archivo JSON con las diferentes configuraciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fcd25b-c64e-4ae4-a1e4-cb35398001fa",
   "metadata": {},
   "source": [
    "## Estructura del proyecto\n",
    "\n",
    "### data/\n",
    "- data.csv\n",
    "\n",
    "\n",
    "### notebooks/\n",
    "0. main.ipynb\n",
    "1. data_preprocessing.ipynb\n",
    "2. graphical_analysis.ipynb\n",
    "3. neural_network_functions.ipynb\n",
    "4. forward_backward_propagation.ipynb\n",
    "5. training_loop.ipynb\n",
    "\n",
    "### src/\n",
    "- ft_functions.py\n",
    "\n",
    "### output/\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39965a23-56d2-49a9-922b-e3cad9ae06fa",
   "metadata": {},
   "source": [
    "## Contenido de los Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8e73d5-dd07-457a-8319-4d2cc5b00f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a80599-dfa3-45ea-b511-5b17a5afa244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1cc2c0a-1a83-4246-b40c-f0238a7a2c2c",
   "metadata": {},
   "source": [
    "https://medium.com/@tutorialcreation81/exploratory-data-analysis-of-breast-cancer-dataset-8c4cd7712f6f\n",
    "\n",
    "https://medium.com/analytics-vidhya/explainable-ai-the-next-level-c6b4dadc240\n",
    "\n",
    "https://www.kaggle.com/code/hanzlanawaz/99-breast-cancer-prediction-using-xgboost\n",
    "\n",
    "\n",
    "'radius_mean', 'texture_mean', 'perimeter_mean',\n",
    "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
    "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
    "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
    "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
    "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
    "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
    "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
    "       'symmetry_worst', 'fractal_dimension_worst'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e86806-19ba-4aee-83ae-b84f5672c57c",
   "metadata": {},
   "source": [
    "# To Do\n",
    "1. Para el archivo normalization.ipynb: almacenamiento de los parámetros de normalización para su uso posterior en la fase de predicción\n",
    "2. Para el archivo preprocesed_data.ipymb:\n",
    "    - División del conjunto de datos en conjuntos de entrenamiento y prueba\n",
    "3. Entrenar con las 30 características y luego probar a reducir la dimensionalidad. Por ejemplo, tomando las 12 principales características que proporciona RandomForestClassifier:  \n",
    "   'feat24', 'feat28', 'feat08', 'feat21', 'feat23', 'feat03', 'feat07', 'feat04', 'feat27', 'feat01', 'feat14', 'feat26'\n",
    "4. Posibles parámetros:\n",
    "    - Porcentaje de división entre datos de entrenamiento y validación  \n",
    "    - Número de capas ocultas\n",
    "    - Número de células por cada capa oculta\n",
    "    - Funciones de activación disponibles\n",
    "    - Learning rate\n",
    "    - Pesos iniciales\n",
    "    - Función de pérdida\n",
    "    - batch size\n",
    "    - epochs\n",
    "    - Métodos de entrenamiento:\n",
    "        - gradient descent\n",
    "        - Nesterov Momentum\n",
    "        - RMSprop\n",
    "        - Adam\n",
    "5. Ejemplo para hacer el proyecto más modular:\n",
    "\n",
    "   ```python\n",
    "   network = model.createNetwork([\n",
    "    layers.denseLayer(input_shape, activation=\"sigmoid\"),\n",
    "    layers.denseLayer(25, activation=\"sigmoid\", weights_initializer=\"he_Uniform\"),\n",
    "    layers.denseLayer(25, activation=\"sigmoid\", weights_initializer=\"he_uniform\"),\n",
    "    layers.denseLayer(25, activation=\"sigmoid\", weights_initializer=\"he_uniform\"),\n",
    "    layers.denseLayer(output_shape, activation=\"softmax\", weights_initializer=\"he_uniform\")\n",
    "    ])\n",
    "\n",
    "    model.fit(network, dt_train, dt_valid, loss=\"binary_crossentropy\", learning_rate=0.0165, batch_size=8, epochs=85)\n",
    "    \n",
    "    ```\n",
    "   \n",
    "6. Evaluar usando la función de error de **entropía cruzada binaria**\n",
    "7. Posibles métricas de evaluación de rendimiento del modelo: precisión, recall, F1-score, AUC, etc.\n",
    "8. Ordenar y documentar los gráficos del archivo de visualización.\n",
    "9. Bonus: Múltiples curvas de aprendizaje en el mismo gráfico\n",
    "10. Bonus: histórico de las métricas obtenidas durante el entrenamiento\n",
    "11. Bonus: implementación de early stopping.\n",
    "12. Bonus: Evaluar la fase de aprendizaje con múltiples métricas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50585977-03ab-4828-b293-e93982081aba",
   "metadata": {},
   "source": [
    "Para el `preporcessing_data.ipynb`:\n",
    "1. Ingeniería de características:\n",
    "    - Selección de características importantes\n",
    "    - Reducción de dimensionalidad si es necesario (PCA, t-SNE, etc.)\n",
    "2. Manejo de desequilibrio de clases:\n",
    "   - Técnicas de sobremuestreo (SMOTE, ADASYN)\n",
    "   - Técnicas de submuestreo\n",
    "   - Ajuste de pesos de clase\n",
    "3. Transformación de datos:\n",
    "   - Aplicación de transformaciones logarítmicas o de potencia\n",
    "   - Manejo de asimetrías en las distribuciones\n",
    "4. Validación cruzada:\n",
    "   - Configuración de la estrategia de validación cruzada\n",
    "   - Preparación de los folds para el entrenamiento\n",
    "5. Guardado de datos procesados:\n",
    "   - Exportación de los conjuntos de entrenamiento y validación\n",
    "   - Guardado de parámetros de preprocesamiento para su uso en datos futuros\n",
    "6. Documentación del proceso:\n",
    "    - Registro de todas las transformaciones aplicadas\n",
    "    - Explicación de las decisiones tomadas en el preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110282f3-e844-4b58-a112-50424ef4c416",
   "metadata": {},
   "source": [
    "## Implementación del Perceptrón Multicapa (MLP)\n",
    "### Contenido de los archivos:\n",
    "\n",
    "1. **5_model_training.ipynb**: Este es el notebook principal para entrenar el modelo. Incluye:\n",
    "   - Carga de datos normalizados\n",
    "   - Implementación completa del MLP con clases modulares\n",
    "   - Funciones de activación (sigmoid, tanh, ReLU, etc.)\n",
    "   - Funciones de pérdida (MSE, binary_crossentropy)\n",
    "   - Métodos de inicialización de pesos (random, xavier, he)\n",
    "   - Optimizadores (SGD, Momentum, Nesterov, RMSprop, Adam)\n",
    "   - Gestión de capas densas y propagación\n",
    "   - Funciones para experimentar con diferentes hiperparámetros\n",
    "\n",
    "2. **utils/neural_network_functions.py**: Archivo con todas las clases y funciones necesarias para el Perceptrón Multicapa:\n",
    "   - Clase `Activation` con diferentes funciones de activación y sus derivadas\n",
    "   - Clase `Loss` con funciones de pérdida y sus derivadas\n",
    "   - Clase `WeightInitializer` para diferentes métodos de inicialización de pesos\n",
    "   - Clase `DenseLayer` para implementar capas densas \n",
    "   - Clase `MultilayerPerceptron` para la red neuronal completa\n",
    "   - Funciones de ayuda para crear y experimentar con diferentes modelos\n",
    "\n",
    "3. **utils/config.py**: Contiene parámetros de configuración para todo el proyecto:\n",
    "   - Rutas de directorios\n",
    "   - Parámetros de normalización\n",
    "   - Parámetros por defecto para el MLP\n",
    "   - Opciones de hiperparámetros para experimentación\n",
    "   - Parámetros de evaluación\n",
    "   - Configuración para visualizaciones\n",
    "\n",
    "4. **utils/evaluation_functions.py**: Funciones para evaluar el rendimiento del modelo:\n",
    "   - Métricas (accuracy, precision, recall, f1-score)\n",
    "   - Matriz de confusión\n",
    "   - Curva ROC y AUC\n",
    "   - Visualizaciones de resultados\n",
    "   - Análisis de experimentos de hiperparámetros\n",
    "\n",
    "### Diseño modular y parametrizable\n",
    "- Estos archivos permiten:\n",
    "\n",
    "1. **Ajustar la arquitectura del modelo**:\n",
    "   - Cambiar el número de capas ocultas (de 1 a 4 o más)\n",
    "   - Ajustar el número de neuronas por capa\n",
    "   - Seleccionar diferentes funciones de activación\n",
    "\n",
    "2. **Modificar el proceso de entrenamiento**:\n",
    "   - Probar diferentes optimizadores\n",
    "   - Ajustar la tasa de aprendizaje\n",
    "   - Cambiar el tamaño del lote\n",
    "   - Configurar early stopping y otros parámetros\n",
    "\n",
    "3. **Evaluar y comparar modelos**:\n",
    "   - Calcular múltiples métricas\n",
    "   - Visualizar curvas de aprendizaje\n",
    "   - Generar informes detallados\n",
    "   - Comparar diferentes configuraciones\n",
    "4. **Flexibilidad en la arquitectura de la red**:\n",
    "   - Número configurable de capas ocultas (de 1 a 4 o más)\n",
    "   - Número personalizable de neuronas por capa\n",
    "   - Múltiples opciones de funciones de activación (sigmoid, ReLU, tanh, etc.)\n",
    "   - Diferentes inicializaciones de pesos (He, Xavier, aleatorio, etc.)\n",
    "\n",
    "5. **Diversas opciones de entrenamiento**:\n",
    "   - Múltiples optimizadores (SGD, Momentum, Nesterov, RMSprop, Adam)\n",
    "   - Parámetros ajustables (learning rate, batch size, epochs)\n",
    "   - Early stopping para evitar sobreajuste\n",
    "   - Mini-batch gradient descent para mayor eficiencia\n",
    "\n",
    "6. **Evaluación exhaustiva**:\n",
    "   - Múltiples métricas de rendimiento generales y específicas para problemas médicos\n",
    "   - Herramientas para encontrar umbrales de decisión óptimos\n",
    "   - Visualizaciones avanzadas (curvas ROC, matrices de confusión con interpretación clínica)\n",
    "   - Generación de informes detallados\n",
    "\n",
    "7. **Documentación completa**:\n",
    "   - Explicaciones detalladas de los conceptos matemáticos y algoritmos\n",
    "   - Visualizaciones para entender las funciones de activación y otros componentes\n",
    "   - Guías de uso y ejemplos prácticos\n",
    "\n",
    "Para usar el código, se pueden importar los módulos en los notebooks y utilizarlos. Por ejemplo:\n",
    "\n",
    "```python\n",
    "# En el notebook de entrenamiento\n",
    "from utils.neural_network_functions import create_mlp, MultilayerPerceptron\n",
    "from utils.config import DEFAULT_MLP_PARAMS\n",
    "from utils.evaluation_functions import calculate_metrics, plot_confusion_matrix\n",
    "\n",
    "# Crear y entrenar modelo\n",
    "model = create_mlp(\n",
    "    input_size=30,  # 30 características\n",
    "    output_size=1,  # Clasificación binaria\n",
    "    hidden_layers=DEFAULT_MLP_PARAMS[\"hidden_layers\"],\n",
    "    neurons_per_layer=DEFAULT_MLP_PARAMS[\"neurons_per_layer\"],\n",
    "    activation=DEFAULT_MLP_PARAMS[\"activation\"],\n",
    "    output_activation=\"sigmoid\",\n",
    "    weights_initializer=\"he_uniform\",\n",
    "    loss=\"binary_crossentropy\"\n",
    ")\n",
    "\n",
    "# Entrenar modelo\n",
    "history = model.fit(X_train, y_train, X_val=X_test, y_val=y_test, \n",
    "                   learning_rate=0.01, optimizer=\"adam\", epochs=100)\n",
    "\n",
    "# Evaluar modelo\n",
    "metrics = calculate_metrics(y_test, model.predict_classes(X_test))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7ae05e-f48a-4163-a678-78a180e75ec4",
   "metadata": {},
   "source": [
    "Continuando con el análisis de los archivos del proyecto.\n",
    "\n",
    "1. **Notebooks principales**:\n",
    "   - `5_model_training.ipynb`: Implementa el entrenamiento del modelo con un MLP completamente parametrizable\n",
    "   - `6_documentation.ipynb`: Documenta en detalle los conceptos teóricos y la implementación\n",
    "   - `7_prediction.ipynb`: Demuestra cómo usar el modelo para hacer predicciones y genera informes clínicos\n",
    "\n",
    "2. **Módulos de utilidades**:\n",
    "   - `utils/neural_network_functions.py`: Implementa todas las clases y funciones del Perceptrón Multicapa\n",
    "   - `utils/config.py`: Contiene configuraciones generales del proyecto\n",
    "   - `utils/evaluation_functions.py`: Funciones generales para evaluar modelos de clasificación\n",
    "   - `utils/cancer_evaluation.py`: Funciones específicas para la evaluación en contexto médico\n",
    "   - `utils/cancer_config.py`: Configuraciones específicas para el conjunto de datos de cáncer\n",
    "\n",
    "\n",
    "### Próximos pasos\n",
    "\n",
    "Si deseamoa seguir desarrollando este proyecto, aquí hay algunas sugerencias:\n",
    "\n",
    "1. **Optimización de hiperparámetros**: Implementar una búsqueda sistemática para encontrar la mejor configuración\n",
    "2. **Regularización avanzada**: Añadir dropout y regularización L2 para mejorar la generalización\n",
    "3. **Validación cruzada**: Implementar k-fold cross-validation para una evaluación más robusta\n",
    "4. **Comparación con otros modelos**: Comparar el rendimiento con otros algoritmos como Random Forest o SVM\n",
    "5. **Análisis de características**: Profundizar en cuáles características son más importantes para el diagnóstico"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
